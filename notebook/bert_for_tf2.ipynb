{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../data/input/train.csv\")\n",
    "test_df = pd.read_csv(\"../data/input/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0     Our Deeds are the Reason of this #earthquake M...       1  \n",
       "1                Forest fire near La Ronge Sask. Canada       1  \n",
       "2     All residents asked to 'shelter in place' are ...       1  \n",
       "3     13,000 people receive #wildfires evacuation or...       1  \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1  \n",
       "...                                                 ...     ...  \n",
       "7608  Two giant cranes holding a bridge collapse int...       1  \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1  \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1  \n",
       "7611  Police investigating after an e-bike collided ...       1  \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Our',\n",
       " 'Deeds',\n",
       " 'are',\n",
       " 'the',\n",
       " 'Reason',\n",
       " 'of',\n",
       " 'this',\n",
       " '#earthquake',\n",
       " 'May',\n",
       " 'ALLAH',\n",
       " 'Forgive',\n",
       " 'us',\n",
       " 'all']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"text\"][0].split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_df[\"text_len\"] = \n",
    "train_df[\"text\"].apply(lambda x: len(x.split(\" \"))).max()\n",
    "#display(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = train_df[\"text\"].apply(lambda x: len(x.split(\" \"))).max()\n",
    "print(MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = train_df.text.values\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "input_ids = [\n",
    "    tokenizer.encode(\n",
    "        sentence, \n",
    "        add_special_tokens=True, \n",
    "        max_length=MAX_LEN,\n",
    "        padding='max_length'\n",
    "    )\n",
    "    for sentence in sentences\n",
    "]\n",
    "labels = train_df.target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual sentense:  All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected\n",
      "Encoded:  [101, 2035, 3901, 2356, 2000, 1005, 7713, 1999, 2173, 1005, 2024, 2108, 19488, 2011, 3738, 1012, 2053, 2060, 13982, 2030, 7713, 1999, 2173, 4449, 2024, 3517, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Actual sentense: \", train_df[\"text\"][2])\n",
    "print(\"Encoded: \", input_ids[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "7613\n",
      "[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "attention_masks = [[float(i > 0) for i in seq] for seq in input_ids]\n",
    "print(type(attention_masks))\n",
    "print(len(attention_masks))\n",
    "print(attention_masks[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2256, 15616, 2024, 1996, 3114, 1997, 2023, 1001, 8372, 2089, 16455, 9641, 2149, 2035, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = sentences[0]\n",
    "\n",
    "tokenizer.encode_plus(\n",
    "    text,\n",
    "    add_special_tokens = True,\n",
    "    max_length = MAX_LEN,\n",
    "    padding = 'max_length', \n",
    "    truncation = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text(sentence):\n",
    "    TAG_RE = re.compile(r'<[^>]+>')\n",
    "    sentence = TAG_RE.sub(' ', sentence)\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweets[0]: Three people died from the heat wave so far\n"
     ]
    }
   ],
   "source": [
    "tweets = []\n",
    "sentences = list(train_df[\"text\"])\n",
    "for sentence in sentences:\n",
    "    tweets.append(preprocess_text(sentence))\n",
    "print(f\"tweets[0]: {tweets[10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "labels = train_df[\"target\"].values\n",
    "print(labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bert\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "bert_layer = hub.KerasLayer(\n",
    "    \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
    "    trainable=False\n",
    ")\n",
    "vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = bert.bert_tokenization.FullTokenizer(vocabulary_file, to_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized_tweets length: 7613\n"
     ]
    }
   ],
   "source": [
    "tokenized_tweets = [\n",
    "    tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text)) for text in tweets\n",
    "]\n",
    "print(f\"tokenized_tweets length: {len(tokenized_tweets)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original[10]: \n",
      " Three people died from the heat wave so far\n",
      "tokenized[10]: \n",
      " [2093, 2111, 2351, 2013, 1996, 3684, 4400, 2061, 2521]\n"
     ]
    }
   ],
   "source": [
    "i = 10\n",
    "print(f\"original[{i}]: \\n {tweets[i]}\")\n",
    "print(f\"tokenized[{i}]: \\n {tokenized_tweets[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretraining Data For Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweets_labels[10]: ([2093, 2111, 2351, 2013, 1996, 3684, 4400, 2061, 2521], 1)\n"
     ]
    }
   ],
   "source": [
    "tweets_labels = [\n",
    "    (tweet, labels[i]) for i, tweet in enumerate(tokenized_tweets)\n",
    "]\n",
    "print(f\"tweets_labels[{i}]: {tweets_labels[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "processed_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: tweets_labels,\n",
    "    output_types=(tf.int32, tf.int32)\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "batched_dataset = processed_dataset.padded_batch(\n",
    "    BATCH_SIZE,\n",
    "    padded_shapes=((None, ), ())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(16, 27), dtype=int32, numpy=\n",
       " array([[ 2256, 15616,  2024,  1996,  3114,  1997,  2023,  8372,  2089,\n",
       "         16455,  9641,  2149,  2035,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [ 3224,  2543,  2379,  2474,  6902,  3351, 21871,  2243,  2710,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [ 2035,  3901,  2356,  2000,  7713,  1999,  2173,  2024,  2108,\n",
       "         19488,  2011,  3738,  2053,  2060, 13982,  2030,  7713,  1999,\n",
       "          2173,  4449,  2024,  3517,     0,     0,     0,     0,     0],\n",
       "        [ 2111,  4374,  3748, 26332, 13982,  4449,  1999,  2662,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [ 2074,  2288,  2741,  2023,  6302,  2013, 10090,  7397,  2004,\n",
       "          5610,  2013,  3748, 26332, 10364,  2015,  2046,  2082,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [ 6857, 10273, 10651,  2662,  1044, 18418,  2701,  1999,  2119,\n",
       "          7826,  2349,  2000,  2697,  2221,  2543, 24689,  7442,  3748,\n",
       "         26332,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [ 7186,  7071,  3082,  4542,  5320,  5956,  9451,  1997,  4534,\n",
       "          1999, 23624, 24826,  5169,  6076,  2752,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [ 1045,  2006,  2327,  1997,  1996,  2940,  1998,  2064,  2156,\n",
       "          2543,  1999,  1996,  5249,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [ 2045,  2019,  5057, 13982,  6230,  2085,  1999,  1996,  2311,\n",
       "          2408,  1996,  2395,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [ 1045,  4452,  2008,  1996, 11352,  2003,  2746,  2000,  2256,\n",
       "          2181,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [ 2093,  2111,  2351,  2013,  1996,  3684,  4400,  2061,  2521,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [ 5292,  3270,  2148,  9925,  2003,  2893, 10361,  5292,  2232,\n",
       "          3524,  2117,  2444,  1999,  2148,  9925,  2054,  2572,  6069,\n",
       "          2079,  2054,  2572,  6069,  2079,  1042, 25465,  2243,  9451],\n",
       "        [24057,  9451,  3516,  9925, 15907,  9925,  2030,  2420,  2310,\n",
       "          2439,  4175,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [ 7186,  1999,  4524,  2080, 12620,  2057,  3369,  4524,  2080,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [ 4053,  2000,  2082,  3902,  2006,  1999,  4800,  2482,  5823,\n",
       "          4911,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [ 2054,  2039,  2158,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0]])>,\n",
       " <tf.Tensor: shape=(16,), dtype=int32, numpy=array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0])>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(batched_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "TOTAL_BATCHES = math.ceil(len(tweets_labels) / BATCH_SIZE)\n",
    "TEST_BATCHES = TOTAL_BATCHES // 10\n",
    "batched_dataset.shuffle(TOTAL_BATCHES)\n",
    "test_data = batched_dataset.take(TEST_BATCHES)\n",
    "train_data = batched_dataset.skip(TEST_BATCHES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "class TEXT_MODEL(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocabulary_size,\n",
    "        embedding_dimensions = 128,\n",
    "        cnn_filters = 50,\n",
    "        dnn_units = 512,\n",
    "        model_output_classes = 2,\n",
    "        dropout_rate = 0.1,\n",
    "        training=False,\n",
    "        name=\"text_model\"\n",
    "    ):\n",
    "        super(TEXT_MODEL, self).__init__(name=name)\n",
    "        \n",
    "        self.embedding = layers.Embedding(\n",
    "            vocabulary_size,\n",
    "            embedding_dimensions\n",
    "        )\n",
    "        self.cnn_layer1 = layers.Conv1D(\n",
    "            filters=cnn_filters,\n",
    "            kernel_size=2,\n",
    "            padding=\"valid\",\n",
    "            activation=\"relu\"\n",
    "        )\n",
    "        self.cnn_layer2 = layers.Conv1D(\n",
    "            filters=cnn_filters,\n",
    "            kernel_size=3,\n",
    "            padding=\"valid\",\n",
    "            activation=\"relu\"\n",
    "        )\n",
    "        self.cnn_layer3 = layers.Conv1D(\n",
    "            filters=cnn_filters,\n",
    "            kernel_size=4,\n",
    "            padding=\"valid\",\n",
    "            activation=\"relu\"\n",
    "        )\n",
    "        self.pool = layers.GlobalMaxPool1D()\n",
    "        self.dense_1 = layers.Dense(units=dnn_units, activation=\"relu\")\n",
    "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
    "        if model_output_classes == 2:\n",
    "            self.last_dense = layers.Dense(\n",
    "                units=1,\n",
    "                activation=\"sigmoid\"\n",
    "            )\n",
    "        else:\n",
    "            self.last_dense = layers.Dense(\n",
    "                units=model_output_classes,\n",
    "                activation=\"softmax\"\n",
    "            )\n",
    "            \n",
    "    def call(self, inputs, training):\n",
    "        l = self.embedding(inputs)\n",
    "        l_1 = self.cnn_layer1(l)\n",
    "        l_1 = self.pool(l_1)\n",
    "        l_2 = self.cnn_layer2(l)\n",
    "        l_2 = self.pool(l_2)\n",
    "        l_3 = self.cnn_layer3(l)\n",
    "        l_3 = self.pool(l_3)\n",
    "        \n",
    "        concatenated = tf.concat([l_1, l_2, l_3], axis=-1)\n",
    "        concatenated = self.dense_1(concatenated)\n",
    "        concatenated = self.dropout(concatenated, training)\n",
    "        model_output = self.last_dense(concatenated)\n",
    "        \n",
    "        return model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "429/429 [==============================] - 19s 44ms/step - loss: 0.6428 - accuracy: 0.6409\n",
      "Epoch 2/5\n",
      "429/429 [==============================] - 16s 37ms/step - loss: 0.3990 - accuracy: 0.8239\n",
      "Epoch 3/5\n",
      "429/429 [==============================] - 16s 38ms/step - loss: 0.1860 - accuracy: 0.9297\n",
      "Epoch 4/5\n",
      "429/429 [==============================] - 17s 40ms/step - loss: 0.1144 - accuracy: 0.9599\n",
      "Epoch 5/5\n",
      "429/429 [==============================] - 16s 38ms/step - loss: 0.0723 - accuracy: 0.9762\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x17c9be1bf48>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_LENGTH = len(tokenizer.vocab)\n",
    "EMB_DIM = 200\n",
    "CNN_FILTERS = 100\n",
    "DNN_UNITS = 256\n",
    "OUTPUT_CLASSES = 2\n",
    "DROPOUT_RATE = 0.2\n",
    "NB_EPOCHS = 5\n",
    "\n",
    "text_model = TEXT_MODEL(\n",
    "    vocabulary_size = VOCAB_LENGTH,\n",
    "    embedding_dimensions=EMB_DIM,\n",
    "    cnn_filters=CNN_FILTERS,\n",
    "    dnn_units=DNN_UNITS,\n",
    "    model_output_classes=OUTPUT_CLASSES,\n",
    "    dropout_rate = DROPOUT_RATE\n",
    ")\n",
    "\n",
    "if OUTPUT_CLASSES == 2:\n",
    "    text_model.compile(\n",
    "        loss=\"binary_crossentropy\",\n",
    "        optimizer=\"adam\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "else:\n",
    "    text_model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        optmizer=\"adam\",\n",
    "        metrics=[\"sparse_categorical_accuracy\"]\n",
    "    )\n",
    "    \n",
    "text_model.fit(train_data, epochs=NB_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 0s 8ms/step - loss: 1.2516 - accuracy: 0.7234\n",
      "loss: 1.2516\n",
      "accuracy: 0.7234\n"
     ]
    }
   ],
   "source": [
    "results = text_model.evaluate(test_data)\n",
    "\n",
    "print(f\"loss: {results[0]:.4f}\")\n",
    "print(f\"accuracy: {results[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_ml)",
   "language": "python",
   "name": "conda_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
