{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "train_df = pd.read_csv(\"../data/input/train.csv\")\n",
    "test_df = pd.read_csv(\"../data/input/test.csv\")\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweetテキストの余計な文字を削除\n",
    "import re\n",
    "\n",
    "def preprocess_text(sentence):\n",
    "    # URL\n",
    "    sentence = re.sub(r\"https?://[\\w/:%#\\$&\\?\\(\\)~\\.=\\+\\-]+\\s*\", ' ', sentence)\n",
    "    # Hash Tag\n",
    "    sentence = re.sub(r'#[^\\s]+\\s*', ' ', sentence)\n",
    "    # アルファベット以外\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "    # 単一文字\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "    # 連続する空白を1つの空白に\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check:   a  b v c <#dog\n",
      "Check:   a  b v c <>#dog # #cat #dog\n",
      "Check:   a  b v c <http://www//141/aa.1a>https://www/w/w//asf1 # \n",
      "Check b c \n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "text = \"Check:   a  b v c <http://www//141/aa.1a>#dog https://www/w/w//asf1 # #cat #dog\"\n",
    "print(re.sub(r'http.* ', '', text))\n",
    "print(re.sub(r\"https?://[\\w/:%#\\$&\\?\\(\\)~\\.=\\+\\-]+\\s*\", '', text))\n",
    "print(re.sub(r'#[^\\s]+\\s*', '', text))\n",
    "print(preprocess_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original[20]: this is ridiculous....\n",
      "tweets[20]: this is ridiculous \n"
     ]
    }
   ],
   "source": [
    "# train dataのテキストを前処理\n",
    "train_tweets = train_df[\"text\"].apply(lambda x: preprocess_text(x))\n",
    "\n",
    "i = 20\n",
    "print(f\"original[{i}]: {train_df['text'][i]}\")\n",
    "print(f\"tweets[{i}]: {train_tweets[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using C:\\Users\\rainc\\AppData\\Local\\Temp\\tfhub_modules to cache modules.\n"
     ]
    }
   ],
   "source": [
    "# bert_tokenizerを作成\n",
    "import bert\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "bert_layer = hub.KerasLayer(\n",
    "    \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
    "    trainable=False\n",
    ")\n",
    "vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = bert.bert_tokenization.FullTokenizer(vocabulary_file, to_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- tokenized_tweets length: 7613\n",
      "- original[10]: \n",
      " Three people died from the heat wave so far\n",
      "- tokenized[10]: \n",
      " [2093, 2111, 2351, 2013, 1996, 3684, 4400, 2061, 2521]\n"
     ]
    }
   ],
   "source": [
    "# bert_tokenizerでtweetをtoken化、ベクトル化\n",
    "tokenized_train_tweets = train_tweets.apply(\n",
    "    lambda x:tokenizer.convert_tokens_to_ids(tokenizer.tokenize(x))\n",
    ")\n",
    "print(f\"- tokenized_tweets length: {len(tokenized_train_tweets)}\")\n",
    "\n",
    "i = 10\n",
    "print(f\"- original[{i}]: \\n {train_tweets[i]}\")\n",
    "print(f\"- tokenized[{i}]: \\n {tokenized_train_tweets[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretraining Data For Training\n",
    "To create sentences of equal length, pad sentence within each batch.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t([tokenized tweet], label)\n",
      "tweets_labels[10]: \t([2093, 2111, 2351, 2013, 1996, 3684, 4400, 2061, 2521], 1)\n"
     ]
    }
   ],
   "source": [
    "# tweetテキスト(tokenized)と正解ラベルのタプルを作る\n",
    "train_labels = train_df[\"target\"].values\n",
    "tweets_labels = [\n",
    "    (tweet, train_labels[i]) for i, tweet in enumerate(tokenized_train_tweets)\n",
    "]\n",
    "print(f\"\\t\\t\\t([tokenized tweet], label)\\ntweets_labels[{i}]: \\t{tweets_labels[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "processed_dataset = tf.data.Dataset.from_generator(\n",
    "    generator=lambda: tweets_labels,# ジェネレータ関数\n",
    "    output_types=(tf.int32, tf.int32)\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# batch毎にpaddingをおこなう\n",
    "batched_dataset = processed_dataset.padded_batch(\n",
    "    BATCH_SIZE,\n",
    "    padded_shapes=((None, ), ())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(16, 26), dtype=int32, numpy=\n",
       " array([[ 2256, 15616,  2024,  1996,  3114,  1997,  2023,  2089, 16455,\n",
       "          9641,  2149,  2035,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [ 3224,  2543,  2379,  2474,  6902,  3351, 21871,  2243,  2710,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [ 2035,  3901,  2356,  2000,  7713,  1999,  2173,  2024,  2108,\n",
       "         19488,  2011,  3738,  2053,  2060, 13982,  2030,  7713,  1999,\n",
       "          2173,  4449,  2024,  3517,     0,     0,     0,     0],\n",
       "        [ 2111,  4374, 13982,  4449,  1999,  2662,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [ 2074,  2288,  2741,  2023,  6302,  2013, 10090,  2004,  5610,\n",
       "          2013, 10364,  2015,  2046,  2082,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [10651,  2662,  1044, 18418,  2701,  1999,  2119,  7826,  2349,\n",
       "          2000,  2697,  2221,  2543,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [ 3082,  4542,  5320,  5956,  9451,  1997,  4534,  1999, 23624,\n",
       "         24826,  5169,  6076,  2752,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [ 1045,  2006,  2327,  1997,  1996,  2940,  1998,  2064,  2156,\n",
       "          2543,  1999,  1996,  5249,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [ 2045,  2019,  5057, 13982,  6230,  2085,  1999,  1996,  2311,\n",
       "          2408,  1996,  2395,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [ 1045,  4452,  2008,  1996, 11352,  2003,  2746,  2000,  2256,\n",
       "          2181,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [ 2093,  2111,  2351,  2013,  1996,  3684,  4400,  2061,  2521,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [ 5292,  3270,  2148,  9925,  2003,  2893, 10361,  5292,  2232,\n",
       "          3524,  2117,  2444,  1999,  2148,  9925,  2054,  2572,  6069,\n",
       "          2079,  2054,  2572,  6069,  2079,  1042, 25465,  2243],\n",
       "        [ 2030,  2420,  2310,  2439,  4175,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [ 1999,  4524,  2080, 12620,  3369,  4524,  2080,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [ 4053,  2000,  2082,  3902,  2006,  1999,  4800,  2482,  5823,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [ 2054,  2039,  2158,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0]])>,\n",
       " <tf.Tensor: shape=(16,), dtype=int32, numpy=array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0])>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(batched_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "TOTAL_BATCHES = math.ceil(len(tweets_labels) / BATCH_SIZE)\n",
    "VALID_BATCHES = TOTAL_BATCHES // 10\n",
    "batched_dataset.shuffle(TOTAL_BATCHES)\n",
    "valid_data = batched_dataset.take(VALID_BATCHES)\n",
    "train_data = batched_dataset.skip(VALID_BATCHES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "\"\"\"\n",
    "    tf.keras.Modelを継承したクラス\n",
    "    Modelクラスを継承して、callメソッドで順伝播を実装することにより、独自のカスタムモデルを作成\n",
    "    Modelクラスの継承APIはKeras2.2.0で導入\n",
    "    下記メソッド、属性は派生したモデルでは使えない\n",
    "        - model.inputs, model.outputs\n",
    "        - model.to_yaml(), model.to_json()\n",
    "        - model.get_config(), model.save()\n",
    "    https://keras.io/ja/models/about-keras-models/\n",
    "\"\"\"\n",
    "class TEXT_MODEL(tf.keras.Model):\n",
    "    # レイヤーの定義\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocabulary_size,\n",
    "        embedding_dimensions = 128,\n",
    "        cnn_filters = 50,\n",
    "        dnn_units = 512,\n",
    "        model_output_classes = 2,\n",
    "        dropout_rate = 0.1,\n",
    "        training=True,\n",
    "        name=\"text_model\"\n",
    "    ):\n",
    "        super(TEXT_MODEL, self).__init__(name=name)\n",
    "        # super(親クラスのオブジェクト, self).親クラスのメソッド\n",
    "        \n",
    "        self.embedding = layers.Embedding(\n",
    "            input_dim=vocabulary_size,\n",
    "            output_dim=embedding_dimensions\n",
    "        )\n",
    "        self.cnn_layer1 = layers.Conv1D(\n",
    "            filters=cnn_filters,\n",
    "            kernel_size=2,\n",
    "            padding=\"valid\",\n",
    "            activation=\"relu\"\n",
    "        )\n",
    "        self.cnn_layer2 = layers.Conv1D(\n",
    "            filters=cnn_filters,\n",
    "            kernel_size=3,\n",
    "            padding=\"valid\",\n",
    "            activation=\"relu\"\n",
    "        )\n",
    "        self.cnn_layer3 = layers.Conv1D(\n",
    "            filters=cnn_filters,\n",
    "            kernel_size=4,\n",
    "            padding=\"valid\",\n",
    "            activation=\"relu\"\n",
    "        )\n",
    "        self.pool = layers.GlobalMaxPool1D()\n",
    "        self.dense_1 = layers.Dense(units=dnn_units, activation=\"relu\")\n",
    "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
    "        \n",
    "        if model_output_classes == 2:\n",
    "            self.last_dense = layers.Dense(\n",
    "                units=1,\n",
    "                activation=\"sigmoid\"\n",
    "            )\n",
    "        else:\n",
    "            self.last_dense = layers.Dense(\n",
    "                units=model_output_classes,\n",
    "                activation=\"softmax\"\n",
    "            )\n",
    "            \n",
    "    # 順伝播を記述\n",
    "    def call(self, inputs, training):\n",
    "        l = self.embedding(inputs)\n",
    "        l_1 = self.cnn_layer1(l)\n",
    "        l_1 = self.pool(l_1)\n",
    "        l_2 = self.cnn_layer2(l)\n",
    "        l_2 = self.pool(l_2)\n",
    "        l_3 = self.cnn_layer3(l)\n",
    "        l_3 = self.pool(l_3)\n",
    "        \n",
    "        concatenated = tf.concat([l_1, l_2, l_3], axis=-1)\n",
    "        concatenated = self.dense_1(concatenated)\n",
    "        concatenated = self.dropout(concatenated, training)\n",
    "        model_output = self.last_dense(concatenated)\n",
    "        \n",
    "        return model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_LENGTH = len(tokenizer.vocab)\n",
    "EMB_DIM = 200\n",
    "CNN_FILTERS = 100\n",
    "DNN_UNITS = 256\n",
    "OUTPUT_CLASSES = 2\n",
    "DROPOUT_RATE = 0.2\n",
    "NB_EPOCHS = 5\n",
    "\n",
    "text_model = TEXT_MODEL(\n",
    "    vocabulary_size = VOCAB_LENGTH,\n",
    "    embedding_dimensions=EMB_DIM,\n",
    "    cnn_filters=CNN_FILTERS,\n",
    "    dnn_units=DNN_UNITS,\n",
    "    model_output_classes=OUTPUT_CLASSES,\n",
    "    dropout_rate = DROPOUT_RATE\n",
    ")\n",
    "\n",
    "if OUTPUT_CLASSES == 2:\n",
    "    text_model.compile(\n",
    "        loss=\"binary_crossentropy\",\n",
    "        optimizer=\"adam\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "else:\n",
    "    text_model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        optmizer=\"adam\",\n",
    "        metrics=[\"sparse_categorical_accuracy\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "429/429 [==============================] - 27s 47ms/step - loss: 0.6668 - accuracy: 0.6141\n",
      "Epoch 2/5\n",
      "429/429 [==============================] - 17s 40ms/step - loss: 0.4580 - accuracy: 0.7965\n",
      "Epoch 3/5\n",
      "429/429 [==============================] - 17s 40ms/step - loss: 0.2291 - accuracy: 0.9104\n",
      "Epoch 4/5\n",
      "429/429 [==============================] - 17s 40ms/step - loss: 0.1238 - accuracy: 0.9539\n",
      "Epoch 5/5\n",
      "429/429 [==============================] - 17s 40ms/step - loss: 0.0879 - accuracy: 0.9720\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x18c21910fc8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_model.fit(train_data, epochs=NB_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 1s 8ms/step - loss: 1.0627 - accuracy: 0.6622\n",
      "loss: 1.0627\n",
      "accuracy: 0.6622\n"
     ]
    }
   ],
   "source": [
    "results = text_model.evaluate(valid_data)\n",
    "\n",
    "print(f\"loss: {results[0]:.4f}\")\n",
    "print(f\"accuracy: {results[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No don like cold \n"
     ]
    }
   ],
   "source": [
    "test_tweets = []\n",
    "for tweet in test_df[\"text\"]:\n",
    "    test_tweets.append(preprocess_text(tweet))\n",
    "print(test_tweets[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2053, 2123, 2066, 3147]\n"
     ]
    }
   ],
   "source": [
    "tokenized_test_tweets = [\n",
    "    tokenizer.convert_tokens_to_ids(tokenizer.tokenize(tweet)) for tweet in test_tweets\n",
    "]\n",
    "print(tokenized_test_tweets[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "X_test = pad_sequences(tokenized_test_tweets, maxlen=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = text_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [1],\n",
       "       ...,\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.where(pred > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       0\n",
       "1   2       1\n",
       "2   3       1\n",
       "3   9       0\n",
       "4  11       1"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission = pd.read_csv(\"../data/input/sample_submission.csv\")\n",
    "sample_submission[\"target\"] = np.where(pred>0.5, 1, 0)\n",
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "dt = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "\n",
    "sample_submission.to_csv(\"../data/output/sub_\" + dt + \"_BerTokenMLP.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_ml)",
   "language": "python",
   "name": "conda_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
