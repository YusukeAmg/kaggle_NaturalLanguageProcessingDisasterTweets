{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv(\"../data/input/train.csv\")\n",
    "test_df = pd.read_csv(\"../data/input/test.csv\")\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"text\"] = train_df[\"keyword\"].fillna(' ') + ' ' + train_df[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweetテキストの余計な文字を削除\n",
    "import re\n",
    "\n",
    "def preprocess_text(sentence):\n",
    "    # URL\n",
    "    sentence = re.sub(r\"https?://[\\w/:%#\\$&\\?\\(\\)~\\.=\\+\\-]+\\s*\", ' ', sentence)\n",
    "    # Hash Tag\n",
    "    sentence = re.sub(r'#[^\\s]+\\s*', ' ', sentence)\n",
    "    # アルファベット以外\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "    # 単一文字\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "    # 連続する空白を1つの空白に\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テキストを前処理\n",
    "train_df[\"text\"] = train_df[\"text\"].apply(lambda x: preprocess_text(x))\n",
    "test_df[\"text\"] = test_df[\"text\"].apply(lambda x: preprocess_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "idx = train_df.index.values\n",
    "idx_train, idx_val = train_test_split(idx, random_state=123)# defaultだと25%がテスト\n",
    "df_train = train_df.loc[idx_train, ['text', 'target']]\n",
    "df_val = train_df.loc[idx_val, ['text', 'target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの準備\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# sentence1がテキスト\n",
    "# sentence2が正解ラベル\n",
    "data = {\n",
    "    \"train\": tf.data.Dataset.from_tensor_slices({\n",
    "        'sentence1': df_train['text'].tolist(),\n",
    "        'sentence2': ['', ] * len(df_train),\n",
    "        'label': df_train['target'].tolist()\n",
    "    }),\n",
    "    \"validation\": tf.data.Dataset.from_tensor_slices({\n",
    "        'sentence1': df_val['text'].tolist(),\n",
    "        'sentence2': ['', ] * len(df_val),\n",
    "        'label': df_val['target'].tolist()\n",
    "    }),\n",
    "    \"test\": tf.data.Dataset.from_tensor_slices({\n",
    "        'sentence1': test_df['text'].tolist(),\n",
    "        'sentence2': ['', ] * len(test_df),\n",
    "        'label': np.zeros(len(test_df), dtype=np.int).tolist()# testデータはデータ数分の0を仮置き\n",
    "    })\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.data.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_map_fn(tokenizer, max_length=100):\n",
    "    \"\"\"map function for pretrained tokenizer\"\"\"\n",
    "    def _tokenize(text_a, text_b, label):\n",
    "        # BertJapaneseTokenizerで\n",
    "        # 「分かち書き」「テキストをidに変換」「token_idsを生成」\n",
    "        inputs = tokenizer.encode_plus(\n",
    "            text_a.numpy().decode('utf-8'),\n",
    "            text_b.numpy().decode('utf-8'),\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            truncation=True\n",
    "        )\n",
    "        input_ids, token_type_ids = inputs[\"input_ids\"], inputs[\"token_type_ids\"]\n",
    "\n",
    "        # Attention_maskを生成\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens.\n",
    "        # Only real tokens are attended to.\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        return input_ids, token_type_ids, attention_mask, label\n",
    "    \n",
    "    def _map_fn(data):\n",
    "        \"\"\"入出力の調整\"\"\"\n",
    "        text_a = data['sentence1']\n",
    "        text_b = data['sentence2']\n",
    "        label = data['label']\n",
    "        out = tf.py_function(\n",
    "            func=_tokenize, \n",
    "            inp=[text_a, text_b, label],\n",
    "            Tout=(tf.int32, tf.int32, tf.int32, tf.int32)\n",
    "        )\n",
    "        return (\n",
    "            {\"input_ids\": out[0], \"token_type_ids\": out[1], \"attention_mask\": out[2]},\n",
    "            out[3]\n",
    "        )\n",
    "    return _map_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset, tokenizer, model from pretrained vocabulary\n",
    "\n",
    "def load_dataset(data, tokenizer, max_length=128, train_batch=8, val_batch=32):\n",
    "    # Prepare dataset for BERT as a tf.data.Dataset instance\n",
    "    train_dataset = data['train'].map(\n",
    "        tokenize_map_fn(tokenizer, max_length=max_length)\n",
    "    )\n",
    "    \n",
    "    valid_dataset = data['validation'].map(\n",
    "        tokenize_map_fn(tokenizer, max_length=max_length)\n",
    "    )\n",
    "\n",
    "    \"\"\"\n",
    "        tf.data.Dataset.padded_batch(\n",
    "            self,\n",
    "            batch_size, \n",
    "            padded_shapes=None,\n",
    "            padding_values=None,\n",
    "            drop_remainder=False\n",
    "        )\n",
    "        paddingとbatch化を同時に行う。\n",
    "        drop_reminder=Trueでbatch化したときにきりよくバッチサイズに達しなかった。\n",
    "        iterationの最後のデータを使用しなくなる。\n",
    "        padded_shapeでpaddingするサイズ(=最大長)を指定。\n",
    "        padded_shapeを指定しないとバッチ毎の最大長にpaddingされる。\n",
    "    \"\"\"\n",
    "    train_dataset = train_dataset.shuffle(100).padded_batch(\n",
    "        batch_size=train_batch, \n",
    "        padded_shapes=(\n",
    "            {'input_ids': max_length, 'token_type_ids': max_length, 'attention_mask': max_length}, \n",
    "            []\n",
    "        ), \n",
    "        drop_remainder=True\n",
    "    )\n",
    "    \n",
    "    valid_dataset = valid_dataset.padded_batch(\n",
    "        batch_size=val_batch, \n",
    "        padded_shapes=(\n",
    "            {'input_ids': max_length, 'token_type_ids': max_length, 'attention_mask': max_length},\n",
    "            []\n",
    "        ),\n",
    "        drop_remainder=True\n",
    "    )\n",
    "    \n",
    "    # prefetch: CPUとGPU/TPUでそれぞれ並列処理\n",
    "    train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    valid_dataset = valid_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return train_dataset, valid_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# fine tuning用に層を追加する関数\n",
    "def make_model(bert, num_classes, max_length, bert_frozen=True):\n",
    "    # use only first layer and froze it\n",
    "    bert.layers[0].trainable = not bert_frozen\n",
    "\n",
    "    # input\n",
    "    input_ids = Input(\n",
    "        shape=(max_length, ), \n",
    "        dtype='int32', \n",
    "        name='input_ids'\n",
    "    )\n",
    "    attention_mask = Input(\n",
    "        shape=(max_length, ), \n",
    "        dtype='int32', \n",
    "        name='attention_mask'\n",
    "    )\n",
    "    token_type_ids = Input(\n",
    "        shape=(max_length, ), \n",
    "        dtype='int32', \n",
    "        name='token_type_ids'\n",
    "    )\n",
    "    inputs = [input_ids, attention_mask, token_type_ids]\n",
    "\n",
    "    # bert\n",
    "    x = bert.layers[0](inputs)\n",
    "    # x: sequence_output, pooled_output\n",
    "    # 2種類の出力がある\n",
    "\n",
    "    # only use pooled_output\n",
    "    out = x[1]\n",
    "\n",
    "    # fc layer(add layers for transfer learning)\n",
    "    out = Dropout(0.25)(out)\n",
    "    out = Dense(128, activation='relu')(out)\n",
    "    out = Dropout(0.5)(out)\n",
    "    out = Dense(1, activation='sigmoid')(out)\n",
    "    #out = Dense(num_classes, activation='softmax')(out)# 多クラスの場合\n",
    "    return Model(inputs=inputs, outputs=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# コールバックでF1-scoreを計算する\n",
    "# [reference](https://qiita.com/koshian2/items/81abfc0a75ea99f726b9)\n",
    "# [reference](https://blog.shikoan.com/keras-f1score/)\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import numpy as np\n",
    "\n",
    "class F1Callback(Callback):\n",
    "    def __init__(self):\n",
    "        # コンストラクタに保持用の配列を宣言\n",
    "        self.f1s = []\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        # ゼロ除算回避用の極小値eps\n",
    "        eps = np.finfo(np.float32).eps# finfo:mashine limits for floating point types\n",
    "        recall = logs[\"val_true_positives\"] / (logs[\"val_possible_positives\"] + eps)\n",
    "        precision = logs[\"val_true_positives\"] / (logs[\"val_predicted_positives\"] + eps)\n",
    "        f1 = 2 * precision * recall / (precision + recall + eps)\n",
    "        print(\"f1_val(from log):\", f1)\n",
    "        self.f1s.append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# TP\n",
    "def true_positives(y_true, y_pred):\n",
    "    return K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "\n",
    "# TP + FN\n",
    "def possible_positives(y_true, y_pred):\n",
    "    return K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "\n",
    "# TP + FP\n",
    "def predicted_positives(y_true, y_pred):\n",
    "    return K.sum(K.round(K.clip(y_pred, 0, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- bert-base-uncased ----\n",
    "from transformers import TFBertModel, BertTokenizer\n",
    "from tensorflow.keras import optimizers, losses, metrics\n",
    "\n",
    "epochs = 8\n",
    "max_length = 200\n",
    "train_batch = 32\n",
    "val_batch = 32\n",
    "num_classes = df_train[\"target\"].nunique()\n",
    "\n",
    "# Load dataset, toknizer, model, from pretrained vocabulary\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "train_dataset, valid_dataset = load_dataset(\n",
    "    data,\n",
    "    tokenizer, \n",
    "    max_length=max_length,\n",
    "    train_batch=train_batch,\n",
    "    val_batch=val_batch\n",
    ")\n",
    "\n",
    "# define fine-tuning model\n",
    "bert = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "model = make_model(bert, num_classes, max_length)\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(),\n",
    "    loss=\"binary_crossentropy\", \n",
    "    metrics=[\"accuracy\", true_positives, possible_positives, predicted_positives]\n",
    ")\n",
    "# Callbackのインスタンス作成\n",
    "f1cb = F1Callback()\n",
    "\n",
    "# Train & evaluate using tf.keras.Model.fit()\n",
    "history = model.fit(\n",
    "    train_dataset, \n",
    "    epochs=epochs,\n",
    "    validation_data=valid_dataset,\n",
    "    callbacks=[f1cb],\n",
    "    # verbose = 0,\n",
    "    #use_multiprocessing=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_axis = [x for x in range(1,epochs + 1)]\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "ax[0].plot(x_axis, history.history['loss'], label='train')\n",
    "ax[0].plot(x_axis, history.history['val_loss'], label='valid')\n",
    "ax[0].set_xlabel('epoch')\n",
    "ax[0].set_ylabel('loss')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(x_axis, history.history['accuracy'], label='train')\n",
    "ax[1].plot(x_axis, history.history['val_accuracy'], label='valid')\n",
    "ax[1].plot(x_axis, np.array(f1cb.f1s), label=\"f1_val score\")\n",
    "ax[1].set_xlabel('epoch')\n",
    "ax[1].set_ylabel('accuracy')\n",
    "ax[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = data['test'].map(\n",
    "    tokenize_map_fn(tokenizer, max_length=max_length)\n",
    ")\n",
    "test_dataset = test_dataset.padded_batch(\n",
    "    batch_size=1, \n",
    "    padded_shapes=(\n",
    "        {'input_ids': max_length, 'token_type_ids': max_length, 'attention_mask': max_length},\n",
    "        []\n",
    "    ),\n",
    "    drop_remainder=True\n",
    ")\n",
    "test_dataset = test_dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       1\n",
       "1   2       0\n",
       "2   3       1\n",
       "3   9       0\n",
       "4  11       1"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission = pd.read_csv(\"../data/input/sample_submission.csv\")\n",
    "sample_submission[\"target\"] = np.where(pred>0.5, 1, 0)\n",
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "dt = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "sample_submission.to_csv(\"../data/output/sub_\" + dt + \"_BERT_finetune.csv\", index=False)\n",
    "model.save_weights(\"../models/model_\" + dt + \"_BERT_finetune_weights\")\n",
    "pickle.dump(history.history, open('../logs/train_score_' + dt + '.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load self-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x20596278548>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---- bert-base-uncased ----\n",
    "from transformers import TFBertModel, BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "bert = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "model = make_model(bert, num_classes, max_length)\n",
    "\n",
    "model.load_weights('../models/model_2021-03-04-08-48-09_BERT_finetune_weights')\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(),\n",
    "    loss=\"binary_crossentropy\", \n",
    "    metrics=[\"accuracy\", true_positives, possible_positives, predicted_positives]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### validデータで予測が誤っているデータを確認する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = tf.data.Dataset.from_tensor_slices({\n",
    "    'sentence1': df_val['text'].tolist(),\n",
    "    'sentence2': ['', ] * len(df_val),\n",
    "    'label': df_val['target'].tolist()\n",
    "})\n",
    "\n",
    "valid_dataset = valid_data.map(\n",
    "    tokenize_map_fn(tokenizer, max_length=max_length)\n",
    ")\n",
    "\n",
    "valid_dataset = valid_dataset.padded_batch(\n",
    "    batch_size=32, \n",
    "    padded_shapes=(\n",
    "        {'input_ids': max_length, 'token_type_ids': max_length, 'attention_mask': max_length},\n",
    "        []\n",
    "    ),\n",
    "    drop_remainder=True# Trueにすると、バッチサイズから余った最後のデータ分がなくなる\n",
    ")\n",
    "\n",
    "pred_val = model.predict(valid_dataset)\n",
    "df_val['pred'] = np.where(pred_val>0.5, 1, 0)# binalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "549     battle Dragon Ball Battle Of Gods Rotten Tomat...\n",
       "415     arsonist Safyuan just minor citation for posse...\n",
       "4704                          landslide while on trip in \n",
       "1717    collided The cars right in front of me collide...\n",
       "6834    trapped Hollywood Movie About Trapped Miners R...\n",
       "                              ...                        \n",
       "5115                                 nuclear reactor Err \n",
       "4838    mass murder JakeGint the mass murder got her h...\n",
       "3544    famine According to Gallup poll the more money...\n",
       "3442    exploded Final update shot on front exploded w...\n",
       "6190    sirens Marketforce Cat Shark wins Sirens round...\n",
       "Name: text, Length: 464, dtype: object"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val[df_val['pred']!=df_val['target']]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
